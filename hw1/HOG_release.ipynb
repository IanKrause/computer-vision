{"cells":[{"cell_type":"markdown","metadata":{"id":"7sgS9mt5r5qW"},"source":["# Collaborators\n","List your collaborators here: None\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"W_Cst4k4tuBc"},"source":["## Initialization\n","\n","Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a PDF for gradescope submission."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2BIzvYlCysit"},"outputs":[],"source":["import math\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import platform\n","import random\n","from random import randrange\n","import time\n","import torch\n","import torchvision\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import torchvision.transforms as transforms\n","import glob\n","from skimage.util import montage\n","\n","np.random.seed(0)"]},{"cell_type":"markdown","metadata":{"id":"mOkFjkLKSjjI"},"source":["# 1. Setup dataset\n","\n","In this section we will download the dataset, unzip it and setup the paths to load images from.\n","\n","This dataset is a tiny subset of [ImageNet](https://www.image-net.org/), a popular dataset for image classification.\n","\n","This tiny dataset has **9538 training** images and **3856 test** images spanning **10 classes** {*fish, English-springer, cassette-player, chain-saw, church, French-horn, garbage-truck, gas-pump, golf-ball, parachute*} stored in the following directory structure:\n","```\n","dataset\n","  ---train\n","     ---class1\n","     ---class2\n","     ...\n","  ---test\n","     ---class1\n","     ---class2\n","     ...       \n","```\n","The data has been cleaned and we have provided dataloading functions below so you can directly use the dataset.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":268},"executionInfo":{"elapsed":16,"status":"error","timestamp":1705615902891,"user":{"displayName":"Erica Weng","userId":"05201697673082993074"},"user_tz":300},"id":"VSHEaEFidInB","outputId":"237b4652-2987-4093-cba2-d2b7a8dce494"},"outputs":[{"name":"stderr","output_type":"stream","text":["'wget' is not recognized as an internal or external command,\n","operable program or batch file.\n","'unzip' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["if not os.path.exists('imagenette'):\n","  #!wget \"https://drive.google.com/uc?export=download&id=1t3XtxcpVwZnKhsM95Q89MxYNlX5mj6aJ&confirm=t\" -O /content/imagenette.zip\n","  !wget https://www.cs.cmu.edu/~deva/data/imagenette.zip -O /content/imagenette.zip\n","  !unzip -qq \"/content/imagenette.zip\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"F2EzFqpmhcDh"},"outputs":[],"source":["train_data_path = '/content/imagenette/train'\n","test_data_path = '/content/imagenette/test'\n","train_image_paths = [] #to store image paths in list\n","test_image_paths  = []\n","classes           = []\n","\n","#Get all the paths from train_data_path and append image paths and class to to respective lists\n","\n","for data_path in glob.glob(train_data_path + '/*'):\n","    classes.append(data_path.split('/')[-1])\n","    train_image_paths.append(glob.glob(data_path + '/*'))\n","\n","for data_path in glob.glob(test_data_path + '/*'):\n","    test_image_paths.append(glob.glob(data_path + '/*'))\n","\n","train_image_paths = list(sum(train_image_paths,[]))\n","random.shuffle(train_image_paths)\n","test_image_paths = list(sum(test_image_paths,[]))\n","random.shuffle(test_image_paths)\n","\n","idx_to_class = {i:j for i, j in enumerate(classes)}\n","class_to_idx = {value:key for key,value in idx_to_class.items()}"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"kHoGxD8RZPy3"},"outputs":[],"source":["def LoadData(img_paths,img_size,class_to_idx):\n","  n = len(img_paths)\n","  Images = np.zeros((n,img_size,img_size,3),dtype='uint8')\n","  Labels = np.zeros(n)\n","  for i in range(n):\n","    path = img_paths[i]\n","    Images[i,:,:,:] = np.asarray(Image.open(path).resize((img_size,img_size)));\n","    Labels[i] = class_to_idx[path.split('/')[-2]]\n","  return Images, Labels\n","\n","# Load images as size 32x32; you can try with img_size = 64 to check if it improves the accuracy\n","img_size = 32\n","Train_Images, Train_Labels = LoadData(train_image_paths, img_size, class_to_idx)\n","Test_Images,   Test_Labels = LoadData( test_image_paths, img_size, class_to_idx)\n"]},{"cell_type":"markdown","metadata":{"id":"woiuWaEGgmJW"},"source":[]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":884,"status":"ok","timestamp":1705355945051,"user":{"displayName":"Deva Ramanan","userId":"07940745110004620261"},"user_tz":300},"id":"CLQDTEvHc-l5","outputId":"e5ab6380-9f16-4c43-add4-720ce9d1a901"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Ian Krause\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\skimage\\util\\_montage.py:117: RuntimeWarning: Mean of empty slice.\n","  fill = arr_in.mean(axis=(0, 1, 2))\n","c:\\Users\\Ian Krause\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\_methods.py:182: RuntimeWarning: invalid value encountered in divide\n","  ret = um.true_divide(\n"]},{"ename":"KeyError","evalue":"0","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(montage(Train_Images[ind[:\u001b[38;5;241m5\u001b[39m],:],grid_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m5\u001b[39m),channel_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m);\n\u001b[1;32m----> 8\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[43midx_to_class\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n","\u001b[1;31mKeyError\u001b[0m: 0"]},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAf0AAAB1CAYAAABEWDiJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAACCklEQVR4nO3cMQrDQAwAwVzw/7+sfOHAnA3ZmVqFukWN1szMBwD4e9+3FwAAniH6ABAh+gAQIfoAECH6ABAh+gAQIfoAECH6ABBx7Q6utU7uAQDcsPNrz6UPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABHX7uDMnNwDADjMpQ8AEaIPABGiDwARog8AEaIPABGiDwARog8AEaIPABGiDwARP1gdCudt7vkuAAAAAElFTkSuQmCC","text/plain":["<Figure size 1500x1500 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Visualize the first 5 images of the 10 classes\n","plt.figure(figsize=(15,15))\n","for i in range(10):\n","  plt.subplot(10,1,i+1)\n","  ind = np.nonzero(Train_Labels == i)[0]\n","  plt.imshow(montage(Train_Images[ind[:5],:],grid_shape=(1,5),channel_axis=3))\n","  plt.axis('off');\n","  plt.title(idx_to_class[i])"]},{"cell_type":"markdown","metadata":{"id":"n7IeaDKNWDdB"},"source":["## Debug Flag\n","Set the debug flag to true when testing.\n","Setting the debug flag to true will let the dataloader use only 20% of the training dataset, which makes everything run faster. This will make testing the code easier.\n","\n","Once you finish the coding part please make sure to change the flag to False and rerun all the cells. This will make the colab ready for submission."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1705355945051,"user":{"displayName":"Deva Ramanan","userId":"07940745110004620261"},"user_tz":300},"id":"PSJFucK5YG26","outputId":"cec305d3-71a6-424f-878f-dbcedd6b5622"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["DEBUG = True\n","\n","# Take a smaller subset of the training set for efficient execution of kNN\n","# We also create a small validation set\n","\n","if DEBUG:\n","  num_train = 1900\n","  num_test =  700\n","else:\n","  num_train = 9000\n","  num_test =  3856\n","\n","X_train = Train_Images[:num_train].reshape(num_train,-1).astype('float64')\n","y_train = Train_Labels[:num_train]\n","X_test  = Test_Images[:num_test].reshape(num_test,-1).astype('float64')\n","y_test  = Test_Labels[:num_test]\n","\n","print('Train data shape: '  , X_train.shape)\n","print('Train labels shape: ', y_train.shape)\n","print('Test data shape: '   , X_test.shape)\n","print('Test labels shape: ' , y_test.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"8THuRKUIWDjO"},"source":["## **Problem 3.1**"]},{"cell_type":"markdown","metadata":{"id":"qp8Gf7Huay6h"},"source":["### (a) Define the KNearestNeighbor class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yiGqWFqgXZGV"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["from collections import Counter\n","class KNearestNeighbor(object):\n","  \"\"\" a kNN classifier with L2 distance \"\"\"\n","\n","  def __init__(self):\n","    pass\n","\n","  def train(self, X, y):\n","    \"\"\"\n","    Train the classifier. For k-nearest neighbors this is just\n","    memorizing the training data.\n","    Inputs:\n","    - X: A numpy array of shape (num_train, D) containing the training data\n","      consisting of num_train samples each of dimension D.\n","    - y: A numpy array of shape (N,) containing the training labels, where\n","         y[i] is the label for X[i].\n","    \"\"\"\n","    self.X_train = X\n","    self.y_train = y\n","\n","  def predict(self, X, k=1, num_loops=0):\n","    \"\"\"\n","    Predict labels for test data using this classifier.\n","    Inputs:\n","    - X: A numpy array of shape (num_test, D) containing test data consisting\n","         of num_test samples each of dimension D.\n","    - k: The number of nearest neighbors that vote for the predicted labels.\n","    - num_loops: Determines which implementation to use to compute distances\n","      between training points and testing points.\n","    Returns:\n","    - y: A numpy array of shape (num_test,) containing predicted labels for the\n","      test data, where y[i] is the predicted label for the test point X[i].\n","    \"\"\"\n","    if num_loops == 0:\n","      dists = self.compute_distances_no_loops(X)\n","    elif num_loops == 1:\n","      dists = self.compute_distances_one_loop(X)\n","    elif num_loops == 2:\n","      dists = self.compute_distances_two_loops(X)\n","    else:\n","      raise ValueError('Invalid value %d for num_loops' % num_loops)\n","\n","    return self.predict_labels(dists, k=k)\n","\n","  def compute_distances_two_loops(self, X):\n","    \"\"\"\n","    Compute the l2 distance between each test point in X and each training point\n","    in self.X_train using a nested loop over both the training data and the\n","    test data.\n","    Inputs:\n","    - X: A numpy array of shape (num_test, D) containing test data.\n","    Returns:\n","    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n","      is the Euclidean distance between the ith test point and the jth training\n","      point.\n","    \"\"\"\n","    num_test = X.shape[0]\n","    num_train = self.X_train.shape[0]\n","    dists = np.zeros((num_test, num_train))\n","    for i in range(num_test):\n","      for j in range(num_train):\n","\n","        # ===== your code here! =====\n","\n","        # TODO:\n","        # Compute the l2 distance between the ith test image and the jth\n","        # training image, and store the result in dists[i, j].\n","\n","        # ==== end of code ====\n","\n","    return dists\n","\n","  def compute_distances_one_loop(self, X):\n","    \"\"\"\n","    Compute the l2 distance between each test point in X and each training point\n","    in self.X_train using a single loop over the test data.\n","    Input / Output: Same as compute_distances_two_loops\n","    \"\"\"\n","    num_test = X.shape[0]\n","    num_train = self.X_train.shape[0]\n","    dists = np.zeros((num_test, num_train))\n","    for i in range(num_test):\n","\n","      # ===== your code here! =====\n","\n","      # TODO:\n","      # Compute the l2 distance between the ith test point and all training\n","      # points, and store the result in dists[i, :].\n","\n","      # ==== end of code ====\n","\n","    return dists\n","\n","  def compute_distances_no_loops(self, X):\n","    \"\"\"\n","    Compute the l2 distance between each test point in X and each training point\n","    in self.X_train using no explicit loops.\n","    Input / Output: Same as compute_distances_two_loops\n","    \"\"\"\n","    num_test = X.shape[0]\n","    num_train = self.X_train.shape[0]\n","    dists = np.zeros((num_test, num_train))\n","\n","    # ===== your code here! =====\n","\n","    # TODO:\n","    # Compute the l2 distance between all test points and all training\n","    # points without using any explicit loops, and store the result in\n","    # dists.\n","    #\n","    # You should implement this function using only basic array operations;\n","    # in particular you should not use functions from scipy.\n","    #\n","    # HINT: ||x - y||^2 = ||x||^2 + ||y||^2 - 2x y^T\n","\n","    # ==== end of code ====\n","\n","    return dists\n","\n","  def predict_labels(self, dists, k=1):\n","    \"\"\"\n","    Given a matrix of distances between test points and training points,\n","    predict a label for each test point.\n","    Inputs:\n","    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n","      gives the distance betwen the ith test point and the jth training point.\n","    Returns:\n","    - y: A numpy array of shape (num_test,) containing predicted labels for the\n","      test data, where y[i] is the predicted label for the test point X[i].\n","    - knn_idxs: List of arrays, containing Indexes of the k nearest neighbors\n","      for the test data. So, for num_tests, it will be a list of length\n","      num_tests with each element of the list, an array of size 'k'. This will\n","      be used for visualization purposes later.\n","    \"\"\"\n","    num_test = dists.shape[0]\n","    y_pred = np.zeros(num_test)\n","    knn_idxs = []\n","    for i in range(num_test):\n","      # A list of length k storing the labels of the k nearest neighbors to\n","      # the ith test point.\n","\n","      closest_y = []\n","\n","      # ===== your code here! =====\n","\n","      # TODO:\n","      # Use the distance matrix to find the k nearest neighbors of the ith\n","      # testing element, and use self.y_train to find the labels of these\n","      # neighbors. Store these labels in closest_y.\n","      # Also, don't forget to apprpriately store indices knn_idxs list.\n","      # Hint: Look up the function numpy.argsort.\n","\n","      top_k_indx = np.argsort(dists[i])[:k]\n","      closest_y = self.y_train[top_k_indx]\n","      knn_idxs.append(top_k_indx)\n","\n","      # ==== end of code ====\n","\n","      # Now that you have found the labels of the k nearest neighbors, the code\n","      # below finds the most common label in the list closest_y of labels.\n","      # and stores this label in y_pred[i]. We break ties by choosing the\n","      # smaller label.\n","\n","      vote = Counter(closest_y)\n","      count = vote.most_common()\n","      y_pred[i] = count[0][0]\n","\n","    return y_pred, knn_idxs"]},{"cell_type":"markdown","metadata":{"id":"T82gncH_PpB4"},"source":["### (b) Check L2 distance implementation\n","Now, let's do some checks to see if you have implemented the functions correctly. We will first calculate distances using ***compute_distance_two_loops*** and check accuracy for k=1 and k=3.\n","Then, we will compare the ***compute_distance_one_loop*** and ***compute_distance_no_loop*** with  ***compute_distance_two_loops*** to ensure all results are consistent."]},{"cell_type":"markdown","metadata":{"id":"em41Ep_nOxzu"},"source":["Initialize the KNN Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2lQm_eIZ-Ti"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["classifier = KNearestNeighbor()\n","classifier.train(X_train,y_train)"]},{"cell_type":"markdown","metadata":{"id":"PHeCw3QxREQD"},"source":["Compute the distance between the training and test set.\n","This might take some time to run since we are running the two loops function which is not efficient.\n","\n","**6 to 8 mins for full dataset | 2 to 3 mins for debug dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1foVkfxbHYw"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["dists_two = classifier.compute_distances_two_loops(X_test)"]},{"cell_type":"markdown","metadata":{"id":"o9rDTdtbRKvS"},"source":["Now, let's do some checks to see if you have implemented the functions correctly. We will first calculate the distances using compute_distance_two_loops function and check the accuracies for k=1 and k=3.\n","Then, we will compare the compute_distance_one_loop and compute_distance_no_loop functions with it to check their correctness.\n","\n","\n","Predict labels and check accuracy for k = 1.\n","You should expect to see approximately 28% accuracy for full dataset.  \n","**(Accuracy below 24% on full dataset (Debug = False) will not be given full grades)**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":139,"status":"ok","timestamp":1705355957260,"user":{"displayName":"Deva Ramanan","userId":"07940745110004620261"},"user_tz":300},"id":"ZIQYEVNpby1C","outputId":"2fdcc02d-1211-4831-e1f5-5307a3917fd8"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["y_test_pred, k_idxs  = classifier.predict_labels(dists_two, k=1)\n","# Compute and print the fraction of correctly predicted examples\n","num_correct = np.sum(y_test_pred == y_test)\n","accuracy = float(num_correct) / num_test\n","print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"]},{"cell_type":"markdown","metadata":{"id":"chBnymxWfeVp"},"source":["Now lets check the one loop implementation. This should also take some time to run.  \n","**4 to 6 mins for full dataset | 1 to 2 mins for debug dataset**\n","\n","**Note:** This function can possibly take a little more time that two loop implementaion because of some quirks in python, numpy and cpu processing. It is fine as long as the final output shows no difference below."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26793,"status":"ok","timestamp":1705355984051,"user":{"displayName":"Deva Ramanan","userId":"07940745110004620261"},"user_tz":300},"id":"k-a5UQUyVFqt","outputId":"f08c9698-989c-4dd4-e62f-522aac1d4eb0"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["# Implement the function compute_distances_one_loop in KNearestNeighbor class\n","# and run the code below:\n","dists_one = classifier.compute_distances_one_loop(X_test)\n","\n","# To ensure that our vectorized implementation is correct, we make sure that it\n","# agrees with the naive implementation. There are many ways to decide whether\n","# two matrices are similar; one of the simplest is the Frobenius norm. In case\n","# you haven't seen it before, the Frobenius norm of two matrices is the square\n","# root of the squared sum of differences of all elements; in other words, reshape\n","# the matrices into vectors and compute the Euclidean distance between them.\n","\n","difference = np.linalg.norm(dists_two - dists_one, ord='fro')\n","print('Difference was: %f' % (difference, ))\n","if difference < 0.001:\n","  print('Good! The distance matrices are the same')\n","else:\n","  print('Uh-oh! The distance matrices are different')"]},{"cell_type":"markdown","metadata":{"id":"4S2XdYq_ftrB"},"source":["Now lets check the vectorized implementation. This should take less than 30 secs to run for full dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":294,"status":"ok","timestamp":1705355984327,"user":{"displayName":"Deva Ramanan","userId":"07940745110004620261"},"user_tz":300},"id":"AqDyi4LDVmmu","outputId":"33d742f6-2411-4bae-f4c2-09eb433f04f7"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["# Now implement the fully vectorized version inside compute_distances_no_loops\n","# and run the code\n","dists_no = classifier.compute_distances_no_loops(X_test)\n","# check that the dist ance matrix agrees with the one we computed before:\n","difference = np.linalg.norm(dists_two - dists_no, ord='fro')\n","print('Difference was: %f' % (difference, ))\n","if difference < 0.001:\n","  print('Good! The distance matrices are the same')\n","else:\n","  print('Uh-oh! The distance matrices are different')"]},{"cell_type":"markdown","metadata":{"id":"gQLoOPEiLEuU"},"source":["Let's compare how fast the implementations are\n","You should see significantly faster performance with the fully vectorized implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40833,"status":"ok","timestamp":1705356025157,"user":{"displayName":"Deva Ramanan","userId":"07940745110004620261"},"user_tz":300},"id":"iCNADI70VuCK","outputId":"d206f9bf-cfe9-4317-f191-acb89fa80ccb"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["def time_function(f, *args):\n","    \"\"\"\n","    Call a function f with args and return the time (in seconds) that it took to execute.\n","    \"\"\"\n","    import time\n","    tic = time.time()\n","    f(*args)\n","    toc = time.time()\n","    return toc - tic\n","\n","two_loop_time = time_function(classifier.compute_distances_two_loops,X_test)\n","print('Two loop version took %f seconds' % two_loop_time)\n","\n","one_loop_time = time_function(classifier.compute_distances_one_loop,X_test)\n","print('One loop version took %f seconds' % one_loop_time)\n","\n","no_loop_time = time_function(classifier.compute_distances_no_loops,X_test)\n","print('No loop version took %f seconds' % no_loop_time)\n","\n","# you should see significantly faster performance with the fully vectorized implementation"]},{"cell_type":"markdown","metadata":{"id":"qtzrh5_P8PdZ"},"source":["## From this point on, we will use the efficient no loop implementation\n","\n","The given accuracy of 29% is much better than chance accuracy of\n","\n","# ===== your answer here! =====\n","\n","\n","# ===== end of your answer =====\n","\n","\n","Though the no-loop impementation is far faster, there maybe situations where one_loop or two_loop implementations are useful, such as [HINT: Imagine really large training set and or testset]\n","\n","# ===== your answer here! =====\n","\n","# ===== end of your answer =====\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":171,"status":"ok","timestamp":1705356025313,"user":{"displayName":"Deva Ramanan","userId":"07940745110004620261"},"user_tz":300},"id":"-_0w7GTS8Ybc","outputId":"c564a2fb-73e4-45a4-93e9-89a657a0086a"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["y_test_pred, k_idxs  = classifier.predict_labels(dists_no, k=3)\n","# Compute and print the fraction of correctly predicted examples\n","num_correct = np.sum(y_test_pred == y_test)\n","accuracy = float(num_correct) / num_test\n","print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"]},{"cell_type":"markdown","metadata":{"id":"3VfFabTwRnxX"},"source":["### Visualize KNN results"]},{"cell_type":"markdown","metadata":{"id":"ELiPYRHkRvzy"},"source":["Let's visualize the K nearest images for some randomly selected examples from the test set using the k_idxs list you returned in predict_labels.  \n","\n","Here the leftmost column is the input image from the test set and rest of the\n","columns are the K nearest neighbors from the training set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":369},"executionInfo":{"elapsed":480,"status":"ok","timestamp":1705356025792,"user":{"displayName":"Deva Ramanan","userId":"07940745110004620261"},"user_tz":300},"id":"zoeIUHXfiXy7","outputId":"32148ad9-ca80-4bab-dcb9-eff300a52391"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["def visualize_knn(classifier,X_test,N=5, K=7):\n","  # This visualization routine makes use of GLOBAL Train_Images and Test_Images variables\n","  # to visualize the K nearest neighbors of the first N Test Images\n","\n","  dist = classifier.compute_distances_no_loops(X_test[:N,:])\n","  _, k_idxs = classifier.predict_labels(dist,k=K)\n","  k_idxs  = np.vstack(k_idxs)\n","  testim  = montage(Test_Images[:N,:],grid_shape=(N,1),channel_axis=3)\n","  trainim = montage(Train_Images[k_idxs.ravel(),:],grid_shape=(N,K),channel_axis=3)\n","  plt.imshow(np.concatenate((testim,trainim),axis=1))\n","  plt.axis('off');\n","  plt.title('Test [leftmost column], K_neighbors [right columns]');\n","\n","visualize_knn(classifier,X_test)\n"]},{"cell_type":"markdown","metadata":{"id":"Z4ODTi-fExOS"},"source":["\n","### Normalizing image descriptors:"]},{"cell_type":"markdown","metadata":{"id":"1F34XK7n3scP"},"source":["Let us try normalizing each image here by subtracting by its mean and scaling to have unit norm."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":159,"status":"ok","timestamp":1705356025948,"user":{"displayName":"Deva Ramanan","userId":"07940745110004620261"},"user_tz":300},"id":"z7QF8kT9sDBY","outputId":"40f5b522-65d0-4acf-89de-ad741fe6161d"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["# Normalize each image descriptor to have zero-mean and unit-length\n","\n","X_train_norm = X_train\n","X_test_norm  = X_test\n","\n","# ===== your code here! =====\n","# Normalize each image descriptor to have zero-mean and unit-length\n","# If X is the descriptor vector for a given image, then sum_i X[i] = 0 and sum_i X[i]**2 = 1\n","\n","# ===== end of code =====\n","\n","print('Train data shape: ', X_train_norm.shape)\n","print('Test data shape: ', X_test_norm.shape)"]},{"cell_type":"markdown","metadata":{"id":"QZ-5WTEg4Rl1"},"source":["We calculate the accuracies again using k = 1 and k = 3 and see that the accuracies are much better compared to those we obtained without any preprocessing on the images!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":253,"status":"ok","timestamp":1705356026200,"user":{"displayName":"Deva Ramanan","userId":"07940745110004620261"},"user_tz":300},"id":"AlNk0lfQC3GY","outputId":"e26d03df-4294-4ffd-f534-788618bea380"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["classifier = KNearestNeighbor()\n","classifier.train(X_train_norm,y_train)\n","\n","# Classify using the efficient no_loops implementation\n","dists = classifier.compute_distances_no_loops(X_test_norm)\n","y_test_pred, k_labels = classifier.predict_labels(dists, k=3)\n","\n","# Compute and print the fraction of correctly predicted examples\n","num_correct = np.sum(y_test_pred == y_test)\n","accuracy = float(num_correct) / num_test\n","print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"]},{"cell_type":"markdown","metadata":{"id":"9r2AH7rD_dwe"},"source":["Written question: Normalization produces image descriptors that have unit length. Prove that minimizing the euclidean distance of such descriptors is equivalent to maximizing the cosine similarity. Here is an example of latex in markdown that might be helpful: $||x - y||^2 = x^Tx - 2x^Ty + y^Ty$    \n","\n","\n","===== your answer here! =====\n","\n","\n","===== end of your answer =====\n"]},{"cell_type":"markdown","metadata":{"id":"PA9dWA4hgCDW"},"source":["## KNN with HOG\n","The previous parts all directly used raw pixels from input images to compute distances with k-NN. In this part, we will first use the Histogram of Oriented Gradients (HOG) as features for each image. We will use these features with our kNN implementation to find the nearest neighbours. Please read the descriptions and fill in the functions below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBqaD-eCG1vU"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["def compute_gradient(image):\n","  \"\"\"\n","    Computes the gradients in both x and y directions.\n","    Computes the magnitudes of the gradients.\n","    Computes the angles from the gradients and map to range [0, 180 deg].\n","    NOTE: You may *NOT* use np.gradient\n","    Inputs:\n","    - image: A (32,32) numpy array corresponding to a grayscale image\n","             or a (32,32,3) array corresponding to a color image\n","    Returns:\n","    - magnitudes: A numpy array of shape (32, 32) where magnitudes[i, j]\n","      is the magnitude of the gradient at the (i, j) pixel in the input image.\n","    - angles: A numpy array of shape (32, 32) where angles[i, j]\n","      is the angle of the gradient at the (i, j) pixel in the input image.\n","    HINT: First write thefunction assuming a grayscale input and get a final accuracy. Then write the color version.\n","          You may wish to use numpy.take_along_axis()\n","    \"\"\"\n","\n","  # ===== your code here! =====\n","\n","  # TODO:\n","  # Compute the gradients along the rows and columns as two arrays.\n","  # Compute the magnitude as the square root of the sum of the squares of both gradients\n","  # Compute the angles as the inverse tangent of the gradients along the rows and\n","  # the gradients along the columns, and map them to the range [0, 180 deg]\n","\n","\n","  # ==== end of code ====\n","  return magnitudes, angles"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stjK-GOYHEVa"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["def bin_gradient(angles, magnitudes, n_orient, pixels_per_cell):\n","  \"\"\"\n","  Given the gradient orientations and magnitudes of an image, creates\n","  a histogram of orientations weighted by gradient magnitudes\n","  Inputs:\n","  - angles: A numpy array of shape (32, 32) where angles[i,j]\n","      is the angle of the gradient at the (i,j) pixel in the input image.\n","  - magnitudes: A numpy array of shape (32, 32) where magnitudes[i,j]\n","      is the magnitude of the gradient at the (i,j) pixel in the input image.\n","  - n_orient: An int representing the number of orientations to bin in histogram\n","  - pixels_per_cell: An int representing the number of rows/columns of pixels\n","      in each spatial cell\n","  Returns:\n","  - oriented_histogram: A numpy array of shape (32/4=8, 32/4=8,9)\n","      for pixels_per_cell=4 and n_orient=9\n","   \"\"\"\n","\n","  n_y,n_x = angles.shape\n","  oriented_histogram = np.zeros((int(n_y//pixels_per_cell),int(n_x//pixels_per_cell),n_orient))\n","\n","  # ===== your code here! =====\n","\n","  # TODO:\n","  # Iterate through each pixel in every cell\n","  # Find the index to the bin in histogram for that pixel's orientation\n","  # Add the weighted magnitude to the corresponding bins in the histogram\n","\n","  # ==== end of code ====\n","  return oriented_histogram\n"]},{"cell_type":"markdown","metadata":{"id":"0FgC4NIE0kNw"},"source":["**NOTE :** Once we create a histogram based on the gradient of the image we need to normalize it. Gradients of an image are sensitive to overall lighting. If you make the image darker by dividing all pixel values by 2, the gradient magnitude will change by half, and therefore the histogram values will change by half.\n","\n","Ideally, we want our image features to be independent of lighting variations. In other words, we would like to “normalize” the histogram so they are not affected by lighting variations.\n","\n","We have provided the normalization code below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8pnHTZemNQl"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["def block_normalize(oriented_histogram, cells_per_block, clip = True, epsilon=1e-5):\n","  \"\"\"\n","  Normalizes the histogram in blocks of size cells_per_block.\n","  Inputs:\n","  - oriented_histogram: A numpy array of shape (num_cell_rows, num_cell_cols, num_orient)\n","    representing the histogram of oriented gradients of the input image.\n","  - cells_per_block: An int representing the number of rows/columns of cells that\n","    should together be normalized in the same block (you can assume )\n","  - clip: If true, this clips the normalized descriptor of each block to ensure that no values are larger than .2 (and then\n","    renormalizes to ensure the clipped descriptor is unit-norm), just as SIFT does\n","  - epsilon: A float indicating the small amount added to the denominator when\n","    normalizing to avoid dividing by zero.\n","  Returns:\n","  - normalized_blocks: A numpy array of shape (num_cell_rows-cells_per_block+1, num_cell_cols+cells_per_block+1,\n","      cells_per_block,cells_per_block,num_orient) where normalized_blocks[i,j] is a normalized [cells_per_block,cells_per_block,num_orient] \"SIFT\" descriptor\n","  \"\"\"\n","\n","  n_blocks_y = oriented_histogram.shape[0]-cells_per_block+1\n","  n_blocks_x = oriented_histogram.shape[1]-cells_per_block+1\n","  normalized_blocks = np.zeros((n_blocks_y,n_blocks_x,cells_per_block,cells_per_block,oriented_histogram.shape[2]))\n","  # ===== your code here! =====\n","\n","  # TODO:\n","  # While there are many ways to compute the descriptor, we suggest iterating through the first dimension (n_blocks_y)\n","  # and second dimension (n_blocks_x) of normalized blocks and compute the [4 4 9] \"SIFT\" descriptor\n","  # (assuming cells_per_block = 4 and n_orient = 9).\n","\n","  # ==== end of code ====\n","  return normalized_blocks"]},{"cell_type":"markdown","metadata":{"id":"ltyfuLeURAd9"},"source":["After implementing your HOG functions, please run the cells below to test the results. You should expect to get an accuracy slightly higher than that with unnormalized raw pixels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SX_CsGmDuj4t"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["def compute_hog(image,n_orient=9,pixels_per_cell=4,cells_per_block=4):\n","  \"\"\"\n","  Builds a Histogram of Oriented Gradients (HOG) weighted by gradient magnitudes\n","  from an input image\n","  Inputs:\n","  - image: A numpy array of shape (32, 32) containing one grayscaled image.\n","  Outputs:\n","  - histogram: A 1D numpy array that represents the HOG descriptor for the image.\n","  \"\"\"\n","  assert(image.dtype == 'float64')\n","  # Read in image and convert to grayscale\n","  # if len(image.shape) > 2:\n","  #  image = np.mean(image,2)\n","\n","  # Compute gradient\n","  magnitudes, angles = compute_gradient(image)\n","\n","  # Bin gradients into cells\n","  oriented_histogram = bin_gradient(angles, magnitudes, n_orient, pixels_per_cell)\n","\n","  # Block normalize the cells\n","  normalized_blocks = block_normalize(oriented_histogram, cells_per_block)\n","\n","  # Return flattened descriptor (without making an additional copy)\n","  return normalized_blocks.ravel()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":548},"executionInfo":{"elapsed":1886,"status":"ok","timestamp":1705356028085,"user":{"displayName":"Deva Ramanan","userId":"07940745110004620261"},"user_tz":300},"id":"IldZm1ga1hJD","outputId":"823f7d8a-5b31-402a-a5c3-e576664d4d27"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["# Check out HOG descriptor for a single image\n","#image = X_train[0].mean(2) # Initially, build representation for grayscale image\n","image = X_train[0].reshape(img_size,img_size,3);\n","plt.figure(figsize=(14,8))\n","plt.subplot(1,3,1)\n","plt.imshow(image.astype('uint8'));\n","plt.axis('off')\n","plt.title('Input Image')\n","\n","pixels_per_cell=4\n","cells_per_block=4\n","n_orient=9\n","angle_step = 180 // n_orient\n","\n","# Step 1: compute gradients\n","magnitudes, angles = compute_gradient(image)\n","\n","plt.subplot(1,3,2)\n","plt.imshow(magnitudes)\n","plt.title('Gradient Magnitudes')\n","\n","plt.subplot(1,3,3)\n","plt.imshow(angles)\n","plt.title('Gradient Orientations')\n","\n","\n","# Step 2: Bin gradients into cells\n","oriented_histogram = bin_gradient(angles, magnitudes, n_orient, pixels_per_cell)\n","plt.figure(figsize=(14,8))\n","#plt.suptitle('Oriented Histograms')\n","for i in range(n_orient):\n","  plt.subplot(1,n_orient,i+1)\n","  plt.imshow(oriented_histogram[:,:,i])\n","\n","# Step 3: Block normalize the cells\n","normalized_blocks = block_normalize(oriented_histogram, cells_per_block)\n"]},{"cell_type":"markdown","metadata":{"id":"RbpG5baPmWUU"},"source":["This part will take some time to run for the full dataset. Approx 1 to 2mins."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5214,"status":"ok","timestamp":1705356033296,"user":{"displayName":"Deva Ramanan","userId":"07940745110004620261"},"user_tz":300},"id":"N0S5eSzd6wA_","outputId":"231b6893-37bc-4885-e563-a1ac2977c281"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["X_train_hog = np.array([compute_hog(X_train[i].reshape(img_size,img_size,3)) for i in range(num_train)])\n","X_test_hog  = np.array([compute_hog( X_test[i].reshape(img_size,img_size,3)) for i in range(num_test)])\n","print('Train data shape: ', X_train_hog.shape)\n","print('Test data shape: ' , X_test_hog.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ywP8WhXCv270"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["classifier = KNearestNeighbor()\n","classifier.train(X_train_hog,y_train)\n","dists = classifier.compute_distances_no_loops(X_test_hog)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":193,"status":"ok","timestamp":1705356034072,"user":{"displayName":"Deva Ramanan","userId":"07940745110004620261"},"user_tz":300},"id":"FmpaoF1QyImO","outputId":"965a7627-70bd-48f5-d459-1219a001dff8"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["# Compute and print the fraction of correctly predicted examples\n","y_test_pred, k_labels  = classifier.predict_labels(dists, k=3)\n","num_correct = np.sum(y_test_pred == y_test)\n","accuracy = float(num_correct) / num_test\n","print('Got %d / %d correct => accuracy: %f' % (num_correct, num_test, accuracy))"]},{"cell_type":"markdown","metadata":{"id":"92-AabhbRcet"},"source":["You can also visualize the K nearest images for some randomly selected examples from the test set using the k_idxs list you returned in predict_labels trained with HOG descriptors."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":369},"executionInfo":{"elapsed":884,"status":"ok","timestamp":1705356034953,"user":{"displayName":"Deva Ramanan","userId":"07940745110004620261"},"user_tz":300},"id":"Jpobfv2c-Slu","outputId":"5f8a283f-eb33-4d66-bd83-c8b10947bec7"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["visualize_knn(classifier,X_test_hog)"]},{"cell_type":"markdown","metadata":{"id":"JFhsM0jqX1HN"},"source":["### Extra credit 1: parameter tweaking\n","Add in descriptions of your optimal parameter settings and the resulting performance, compared to your default parameter settings and your default performance\n","\n","\n"," # ===== your answers here! =====\n","\n","\n"," # ==== end of answer ====\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jsXqhV0JY-pE"},"source":["### Extra credit 2: low-rank descriptors"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"executionInfo":{"elapsed":13062,"status":"ok","timestamp":1705357235736,"user":{"displayName":"Deva Ramanan","userId":"07940745110004620261"},"user_tz":300},"id":"YrIOw8qGY92M","outputId":"ad6dce41-3412-4310-d25f-36284972dd64"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with 'base' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"]}],"source":["# ===== your code here! =====\n","\n","# ===== end of code ====="]}],"metadata":{"colab":{"provenance":[{"file_id":"16CqvA-XrTZkTy9TmAwJkIlNutfknHgVH","timestamp":1694743064778}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
